{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIBIFYPtnQn36/iIwSmXBr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devv-shayan/RAG-APPLICATION-/blob/main/github_rag_application.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0. Install required libraries**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YBqTgiOnqdq1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZwTrYNsqTFt",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade langchain langchain_community langchain-google-genai faiss-cpu sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Load GitHub Repository Documents**\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "First, load markdown documents from a specified GitHub repository. This example uses the panaversity/learn-applied-generative-ai-fundamentals repository, but you can adjust it for your own."
      ],
      "metadata": {
        "id": "H1SnYccCrGAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import GithubFileLoader\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "GITHUB_ACCESS_TOKEN = userdata.get(\"GITHUB_ACCESS_TOKEN\")\n",
        "genai.configure(api_key=GITHUB_ACCESS_TOKEN)"
      ],
      "metadata": {
        "id": "VEQwqSFIrJ1d"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = GithubFileLoader(\n",
        "    repo=\"panaversity/learn-applied-generative-ai-fundamentals\",\n",
        "    branch=\"main\",\n",
        "    access_token=GITHUB_ACCESS_TOKEN,  # Replace with your GitHub access token\n",
        "    github_api_url=\"https://api.github.com\",\n",
        "    file_filter=lambda file_path: file_path.endswith(\".md\")\n",
        ")\n",
        "\n",
        "docs = loader.load()\n",
        "print(f\"Loaded {len(docs)} documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5HXGN2XrTm4",
        "outputId": "d8df924a-2097-464d-c035-a5598f2a6cfe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 128 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Split Documents into Chunks**\n",
        "Use the SpacyTextSplitter or RecursiveCharacterTextSplitter to divide the documents into smaller chunks."
      ],
      "metadata": {
        "id": "P5yo07fauI8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **SpacyTextSplitter**"
      ],
      "metadata": {
        "id": "-G7Cajfcdnxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import SpacyTextSplitter\n",
        "\n",
        "# Install spaCy and download the English model\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Initialize the semantic text splitter\n",
        "text_splitter = SpacyTextSplitter(chunk_size=1000, chunk_overlap=200, pipeline='en_core_web_sm')\n",
        "\n",
        "# Split the documents\n",
        "splits = text_splitter.split_documents(docs)\n",
        "print(f\"Split into {len(splits)} chunks using SpacyTextSplitter.\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "J804Xng3uIRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using RecursiveCharacterTextSplitter**"
      ],
      "metadata": {
        "id": "SbUwdtqzuHA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "ayBh9CdVuB2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "print(f\"Split into {len(splits)} chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Xp_nsEbt_Jm",
        "outputId": "45bb4f7d-13c2-4e92-b0da-85fc1c8f09c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split into 889 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Chunk: {splits[0].page_content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr3njgwGH4Z8",
        "outputId": "7c9e08b0-44d3-4417-cd9c-6ff424185b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk: # Prologue\n",
            "\n",
            "## The $100 Trillion Opportunity\n",
            "\n",
            "**\n",
            "\n",
            "[Watch: The $100 Trillion AI Industrial Revolution by NVIDIA's Jensen Huang](https://www.youtube.com/watch?v=e5Zol4RYq2o)**\n",
            "\n",
            "**[From AlexNet to AI Factories: Nvidia's Vision for a $100 Trillion Generative AI Economy](https://www.linkedin.com/pulse/from-alexnet-ai-factories-nvidias-vision-100-trillion-schwentker-bgt7c/)**\n",
            "\n",
            "**Chip-maker Nvidia became the world's most valuable company after its share price climbed to an all-time high on June 19, 2024.\n",
            "\n",
            "It is now worth $3.34tn (Â£2.63tn), with the price having nearly doubled since the start of this year.\n",
            "\n",
            "**\n",
            "\n",
            "[AI frenzy makes Nvidia the world's most valuable company](https://www.bbc.com/news/articles/cyrr40x0z2mo)\n",
            "\n",
            "!\n",
            "\n",
            "[nvidia price](nvidia.png \"nvidia price\")\n",
            "\n",
            "!\n",
            "\n",
            "[top ten price](topten.png \"top ten price\")\n",
            "\n",
            "Nvidia is ushering in a new industrial revolution driven by generative AI - a fundamentally new way of computing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Create Embeddings using Sentence Transformers**\n",
        "Generate semantic embeddings for each document chunk using the all-MiniLM-L6-v2 model."
      ],
      "metadata": {
        "id": "SraTMsqDuTHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import SentenceTransformerEmbeddings"
      ],
      "metadata": {
        "id": "0-EkxAOKuSFp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "print(\"Embeddings created successfully.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EathtUNQuaOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Embedding dimension: {embeddings}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v-XKEB1Ha2F",
        "outputId": "007e58a5-fa17-4f45-ca82-825854b48ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding dimension: client=SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ") model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Store Embeddings in FAISS**\n",
        "Store the embeddings in a FAISS vector store for efficient document retrieval."
      ],
      "metadata": {
        "id": "YEIIEpQYughY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n"
      ],
      "metadata": {
        "id": "9kV7oim6ulZC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
        "print(f\"Vector store created with {vectorstore.index.ntotal} embeddings.\")\n",
        "print(\"Vector store created using FAISS.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBcW0qPruiyB",
        "outputId": "3698c110-2222-4182-9fb4-316b6140fc0a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store created with 919 embeddings.\n",
            "Vector store created using FAISS.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Initialize a Retriever**\n",
        "Use the FAISS vector store to retrieve relevant documents based on semantic similarity."
      ],
      "metadata": {
        "id": "FPuG_Wv9vx4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 5}  # Retrieve top 5 similar documents\n",
        ")\n",
        "\n",
        "# Retrieve relevant documents\n",
        "# retrieved_docs = retriever.invoke(\"why we are using langchain ?\")\n",
        "# print(f\"Retrieved {len(retrieved_docs)} documents.\")\n",
        "# print(retrieved_docs)"
      ],
      "metadata": {
        "id": "Q-rpFYunvJbM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Initialize the Google Gemini Language Model**\n",
        "Configure the Google Gemini AI to generate language model responses."
      ],
      "metadata": {
        "id": "HVwG_SMgwvk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "genai.configure(api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "ceEJ2oBYwyjl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GEMINI_API_KEY # Replace with your Google API key\n",
        ")\n",
        "print(\"LLM initialized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIQJM_t_wxSO",
        "outputId": "24e77771-4eb5-4bd7-bafa-027d462c45ea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MDWF6Vr1xMb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Create the RAG Chain**\n",
        "Pull a pre-defined prompt from the LangChain hub and create the RAG chain that ties the document retrieval with LLM responses."
      ],
      "metadata": {
        "id": "yX6lQ2pU4Vbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain import hub"
      ],
      "metadata": {
        "id": "bF8Hx3s-xSHF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull prompt from LangChain hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# Create RAG Chain\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever | format_docs,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "2EB5b89lxbqh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970c549a-1f6d-43b8-cd93-0dbdcba135bb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Run the RAG Chain**\n",
        "Ask a question, and the RAG chain will retrieve relevant documents and generate an answer using the language model."
      ],
      "metadata": {
        "id": "KG5Fo9m94fkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke(\"how to make briyani?\")\n",
        "print(\"Response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcDLhk0Sx9CV",
        "outputId": "b7062c61-b246-4caf-e9e6-3540dbff4601"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "I'm sorry, but the context provided does not contain information about how to make biryani. The context focuses on AI-driven instructions for cooking, but it does not include any recipes or instructions for biryani. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "This project demonstrates a scalable Retrieval-Augmented Generation (RAG) pipeline using LangChain, FAISS, and Google Gemini AI. It enables document retrieval from a GitHub repository and uses an LLM to generate responses based on relevant retrieved content."
      ],
      "metadata": {
        "id": "W-amvmWceh5B"
      }
    }
  ]
}